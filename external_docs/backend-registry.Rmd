---
title: "Backend Registry: Extending Data Format Support"
author: "fmridataset Team"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Backend Registry: Extending Data Format Support}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE,
  warning = FALSE,
  message = FALSE
)

suppressPackageStartupMessages({
  library(fmridataset)
})
```

# Motivation: The Format Fragmentation Problem

Imagine you're collaborating on a multi-site neuroimaging study where each lab uses different tools and data formats. Site A stores preprocessed data in custom HDF5 files with specialized metadata, Site B uses BIDS-organized NIfTI files, Site C provides CSV matrices exported from MATLAB, and Site D has data in a proprietary format from their analysis software. Traditional approaches would require you to write completely separate loading and processing code for each format, manually handle different metadata conventions, and constantly switch between different programming interfaces.

The fmridataset backend registry system eliminates this complexity by providing a pluggable architecture where new data formats can be seamlessly integrated into the existing ecosystem. Once a backend is registered, that data format immediately works with all existing analysis code, chunking systems, and study-level operations. This approach transforms the format fragmentation problem from a major barrier into a simple extension task, enabling true format independence in neuroimaging research.

# A Real Example: Creating and Using Custom Backends

Let's dive into a concrete example that shows how to create a custom backend and integrate it seamlessly into the fmridataset ecosystem. We'll create a backend for CSV files that demonstrates all the key concepts:

```{r backend-example}
library(fmridataset)

# Step 1: Create a custom CSV backend
csv_backend <- function(csv_file, mask_file = NULL, ...) {
  # Validate inputs
  if (!file.exists(csv_file)) {
    stop("CSV file does not exist: ", csv_file)
  }
  
  # Read and validate data (simulate for vignette)
  # In practice: data_matrix <- read.csv(csv_file, header = FALSE)
  # For demo, create synthetic data
  set.seed(123)
  data_matrix <- matrix(rnorm(1000), nrow = 100, ncol = 10)
  
  # Handle mask
  if (is.null(mask_file)) {
    # Default mask: all columns are valid
    mask <- rep(TRUE, ncol(data_matrix))
  } else {
    # mask <- as.logical(read.csv(mask_file, header = FALSE)[[1]])
    mask <- rep(TRUE, ncol(data_matrix))  # Simplified for demo
  }
  
  # Create backend object
  backend <- list(
    csv_file = csv_file,
    mask_file = mask_file,
    data_matrix = data_matrix,
    mask = mask,
    spatial_dims = c(ncol(data_matrix), 1, 1),  # Flat 3D space
    is_open = FALSE
  )
  
  class(backend) <- c("csv_backend", "storage_backend")
  backend
}

# Step 2: Implement required S3 methods
backend_open.csv_backend <- function(backend) {
  # CSV backend stores data in memory, so opening just marks state
  backend$is_open <- TRUE
  backend
}

backend_close.csv_backend <- function(backend) {
  backend$is_open <- FALSE
  invisible(NULL)
}

backend_get_dims.csv_backend <- function(backend) {
  list(
    spatial = backend$spatial_dims,
    time = nrow(backend$data_matrix)
  )
}

backend_get_mask.csv_backend <- function(backend) {
  backend$mask
}

backend_get_data.csv_backend <- function(backend, rows = NULL, cols = NULL) {
  # Apply mask first
  data <- backend$data_matrix[, backend$mask, drop = FALSE]
  
  # Apply row subsetting
  if (!is.null(rows)) {
    data <- data[rows, , drop = FALSE]
  }
  
  # Apply column subsetting (after masking)
  if (!is.null(cols)) {
    data <- data[, cols, drop = FALSE]
  }
  
  data
}

backend_get_metadata.csv_backend <- function(backend) {
  list(
    format = "CSV",
    source_file = backend$csv_file,
    mask_file = backend$mask_file,
    data_loaded = backend$is_open
  )
}

# Step 3: Register the backend
register_backend(
  name = "csv",
  factory = csv_backend,
  description = "CSV file backend for simple text-based fMRI data"
)

cat("CSV backend registered successfully\n")
```

Now let's see the backend in action:

```{r backend-usage}
# Step 4: Use the registered backend
# Create a backend instance (would use real file path)
csv_backend_instance <- create_backend("csv", csv_file = "example_data.csv")

# Open the backend
csv_backend_instance <- backend_open(csv_backend_instance)

# Query metadata without loading data
dims <- backend_get_dims(csv_backend_instance)
cat("Data dimensions:", dims$time, "timepoints ×", sum(dims$spatial), "voxels\n")

metadata <- backend_get_metadata(csv_backend_instance)
cat("Data format:", metadata$format, "\n")

# Use in dataset creation
dataset <- fmri_dataset(
  scans = csv_backend_instance,
  TR = 2.0,
  run_length = 100
)

cat("Created dataset using CSV backend\n")
print(dataset)

# All standard operations work
data_matrix <- get_data_matrix(dataset)
cat("Retrieved data matrix with dimensions:", dim(data_matrix), "\n")

# Chunking works automatically
chunks <- data_chunks(dataset, nchunks = 3)
cat("Created", length(chunks), "chunks for processing\n")
```

```{r backend-output, echo=FALSE}
# Expected output:
# CSV backend registered successfully
# Data dimensions: 100 timepoints × 10 voxels
# Data format: CSV
# Created dataset using CSV backend
# fMRI Dataset (csv_backend)
# Data dimensions: 100 timepoints × 10 voxels
# Temporal structure: 1 run (100 timepoints)
# TR: 2 seconds
# Retrieved data matrix with dimensions: 100 10
# Created 3 chunks for processing
```

> **💡 Key Insight**: Once registered, the CSV backend works seamlessly with all existing fmridataset functionality. The same analysis code that works with NIfTI files now works with CSV files without any modifications.

# Understanding the Backend Registry System

The backend registry system is the foundation of fmridataset's extensibility. It provides a clean separation between data storage formats and analysis operations, enabling unlimited format support while maintaining a consistent user interface.

## The Backend Contract

Every backend must implement a standardized contract consisting of six core methods. This contract ensures that all backends behave consistently and can be used interchangeably by the rest of the system. The contract defines not just what methods must exist, but also their expected behavior and error handling patterns.

The backend contract is designed to support both eager and lazy loading strategies. Some backends (like matrix backends) can provide immediate data access, while others (like file backends) can defer loading until absolutely necessary. This flexibility enables optimal performance characteristics for different data sources while maintaining the same programming interface.

Understanding the backend contract is crucial for creating reliable extensions. Each method has specific responsibilities and expected return formats that must be followed exactly. The contract also defines error conditions and how they should be communicated to the rest of the system.

## Registry Architecture

The registry itself is a sophisticated system that manages backend discovery, validation, and instantiation. When you call `register_backend()`, the system performs validation to ensure the backend meets the contract requirements. It also handles method dispatch, ensuring that the correct backend implementation is called for each operation.

The registry supports runtime registration, meaning backends can be added by external packages without modifying the core fmridataset code. This enables a vibrant ecosystem where specialized packages can provide backends for niche formats while leveraging all the existing analysis infrastructure.

The registry also provides introspection capabilities, allowing users and developers to discover available backends, query their capabilities, and understand their specific requirements. This transparency makes the system more approachable and helps with debugging when things go wrong.

## Validation and Error Handling

The registry system includes comprehensive validation to catch common errors during backend development. When a backend is registered, the system checks that all required methods are implemented and that they follow the expected patterns. This validation helps developers catch issues early rather than encountering mysterious errors during analysis.

The validation system also includes runtime checks that ensure backends continue to behave correctly during actual use. These checks help identify issues like resource leaks, inconsistent data formats, or unexpected error conditions that might not be caught during initial development.

Error handling in the registry system is designed to be informative and actionable. When something goes wrong, the system provides detailed error messages that help identify both what went wrong and how to fix it. This approach reduces the debugging time required when developing custom backends.

# Deep Dive: Creating Robust Backends

With the basic concepts established, let's explore how to create production-quality backends that handle real-world complexities and edge cases.

## Complete Backend Implementation

A robust backend implementation goes beyond the basic contract to handle edge cases, provide good error messages, and optimize for performance:

```{r robust-backend}
# Advanced backend with comprehensive features
advanced_csv_backend <- function(csv_file, mask_file = NULL, 
                                 delimiter = ",", has_header = FALSE, ...) {
  # Input validation
  if (!is.character(csv_file) || length(csv_file) != 1) {
    stop("csv_file must be a single character string")
  }
  
  if (!file.exists(csv_file)) {
    stop("CSV file does not exist: ", csv_file)
  }
  
  # Check file size for memory planning
  file_info <- file.info(csv_file)
  if (file_info$size > 1e9) {  # 1GB
    warning("Large CSV file detected (", 
            round(file_info$size / 1e6, 1), "MB). Consider chunked loading.")
  }
  
  # Validate delimiter
  if (!delimiter %in% c(",", ";", "\t", "|")) {
    warning("Unusual delimiter '", delimiter, "' - ensure it's correct")
  }
  
  # Create backend object with metadata
  backend <- list(
    csv_file = csv_file,
    mask_file = mask_file,
    delimiter = delimiter,
    has_header = has_header,
    file_size = file_info$size,
    file_modified = file_info$mtime,
    data_cache = NULL,
    mask_cache = NULL,
    spatial_dims = NULL,
    is_open = FALSE,
    read_count = 0
  )
  
  class(backend) <- c("advanced_csv_backend", "storage_backend")
  backend
}

# Enhanced backend methods with error handling
backend_open.advanced_csv_backend <- function(backend) {
  if (backend$is_open) {
    return(backend)  # Already open
  }
  
  tryCatch({
    # Read data with proper error handling
    # data <- read.csv(backend$csv_file, 
    #                  header = backend$has_header,
    #                  sep = backend$delimiter)
    
    # Simulate data loading for vignette
    set.seed(123)
    data <- matrix(rnorm(2000), nrow = 200, ncol = 10)
    
    # Validate data format
    if (!is.numeric(data)) {
      stop("CSV data must be numeric for fMRI analysis")
    }
    
    if (any(is.na(data))) {
      na_prop <- mean(is.na(data))
      if (na_prop > 0.1) {
        stop("Too many missing values in CSV data (", 
             round(na_prop * 100, 1), "%)")
      }
      warning("CSV contains ", sum(is.na(data)), " missing values")
    }
    
    # Handle mask
    if (!is.null(backend$mask_file)) {
      if (!file.exists(backend$mask_file)) {
        stop("Mask file not found: ", backend$mask_file)
      }
      # mask <- read.csv(backend$mask_file, header = FALSE)[[1]]
      mask <- rep(TRUE, ncol(data))  # Simplified for demo
    } else {
      mask <- rep(TRUE, ncol(data))
    }
    
    # Validate mask
    if (length(mask) != ncol(data)) {
      stop("Mask length (", length(mask), 
           ") does not match data columns (", ncol(data), ")")
    }
    
    # Cache data and metadata
    backend$data_cache <- data
    backend$mask_cache <- as.logical(mask)
    backend$spatial_dims <- c(sum(backend$mask_cache), 1, 1)
    backend$is_open <- TRUE
    backend$read_count <- backend$read_count + 1
    
    cat("Opened CSV backend: ", nrow(data), " timepoints, ", 
        sum(backend$mask_cache), " voxels\n")
    
    return(backend)
    
  }, error = function(e) {
    stop("Failed to open CSV backend: ", conditionMessage(e))
  })
}

backend_close.advanced_csv_backend <- function(backend) {
  # Clear cached data to free memory
  backend$data_cache <- NULL
  backend$mask_cache <- NULL
  backend$is_open <- FALSE
  
  cat("Closed CSV backend, freed cached data\n")
  invisible(NULL)
}

backend_get_dims.advanced_csv_backend <- function(backend) {
  if (!backend$is_open) {
    stop("Backend must be opened before querying dimensions")
  }
  
  list(
    spatial = backend$spatial_dims,
    time = nrow(backend$data_cache)
  )
}

backend_get_mask.advanced_csv_backend <- function(backend) {
  if (!backend$is_open) {
    stop("Backend must be opened before accessing mask")
  }
  
  backend$mask_cache
}

backend_get_data.advanced_csv_backend <- function(backend, rows = NULL, cols = NULL) {
  if (!backend$is_open) {
    stop("Backend must be opened before accessing data")
  }
  
  # Apply mask first
  data <- backend$data_cache[, backend$mask_cache, drop = FALSE]
  
  # Apply subsetting with validation
  if (!is.null(rows)) {
    if (any(rows < 1 | rows > nrow(data))) {
      stop("Row indices out of range: ", 
           paste(range(rows), collapse = "-"), 
           " (data has ", nrow(data), " rows)")
    }
    data <- data[rows, , drop = FALSE]
  }
  
  if (!is.null(cols)) {
    if (any(cols < 1 | cols > ncol(data))) {
      stop("Column indices out of range: ",
           paste(range(cols), collapse = "-"),
           " (masked data has ", ncol(data), " columns)")
    }
    data <- data[, cols, drop = FALSE]
  }
  
  return(data)
}

backend_get_metadata.advanced_csv_backend <- function(backend) {
  list(
    format = "Advanced CSV",
    source_file = backend$csv_file,
    mask_file = backend$mask_file,
    delimiter = backend$delimiter,
    file_size_mb = round(backend$file_size / 1e6, 2),
    file_modified = backend$file_modified,
    is_open = backend$is_open,
    read_count = backend$read_count,
    has_cached_data = !is.null(backend$data_cache)
  )
}

# Register the enhanced backend
register_backend(
  name = "advanced_csv",
  factory = advanced_csv_backend,
  description = "Enhanced CSV backend with comprehensive error handling and validation"
)

cat("Advanced CSV backend registered\n")
```

This enhanced backend demonstrates proper error handling, input validation, and resource management.

## Backend Validation and Testing

A crucial aspect of backend development is comprehensive testing to ensure reliability:

```{r backend-testing}
# Comprehensive backend testing framework
test_backend_contract <- function(backend_name, test_params) {
  cat("Testing backend contract for:", backend_name, "\n")
  
  # Test 1: Backend creation
  tryCatch({
    backend <- do.call(create_backend, c(list(backend_name), test_params))
    cat("✓ Backend creation successful\n")
  }, error = function(e) {
    cat("✗ Backend creation failed:", conditionMessage(e), "\n")
    return(FALSE)
  })
  
  # Test 2: Backend opening
  tryCatch({
    backend <- backend_open(backend)
    cat("✓ Backend opening successful\n")
  }, error = function(e) {
    cat("✗ Backend opening failed:", conditionMessage(e), "\n")
    return(FALSE)
  })
  
  # Test 3: Dimension queries
  tryCatch({
    dims <- backend_get_dims(backend)
    if (!is.list(dims) || !all(c("spatial", "time") %in% names(dims))) {
      stop("Invalid dimension format")
    }
    cat("✓ Dimension queries successful\n")
  }, error = function(e) {
    cat("✗ Dimension queries failed:", conditionMessage(e), "\n")
    return(FALSE)
  })
  
  # Test 4: Mask access
  tryCatch({
    mask <- backend_get_mask(backend)
    if (!is.logical(mask)) {
      stop("Mask must be logical vector")
    }
    cat("✓ Mask access successful\n")
  }, error = function(e) {
    cat("✗ Mask access failed:", conditionMessage(e), "\n")
    return(FALSE)
  })
  
  # Test 5: Data access
  tryCatch({
    full_data <- backend_get_data(backend)
    if (!is.matrix(full_data) || !is.numeric(full_data)) {
      stop("Data must be numeric matrix")
    }
    
    # Test partial data access
    subset_data <- backend_get_data(backend, rows = 1:5, cols = 1:3)
    if (nrow(subset_data) != 5 || ncol(subset_data) != 3) {
      stop("Subsetting failed")
    }
    
    cat("✓ Data access successful\n")
  }, error = function(e) {
    cat("✗ Data access failed:", conditionMessage(e), "\n")
    return(FALSE)
  })
  
  # Test 6: Metadata
  tryCatch({
    metadata <- backend_get_metadata(backend)
    if (!is.list(metadata)) {
      stop("Metadata must be a list")
    }
    cat("✓ Metadata access successful\n")
  }, error = function(e) {
    cat("✗ Metadata access failed:", conditionMessage(e), "\n")
    return(FALSE)
  })
  
  # Test 7: Backend closing
  tryCatch({
    backend_close(backend)
    cat("✓ Backend closing successful\n")
  }, error = function(e) {
    cat("✗ Backend closing failed:", conditionMessage(e), "\n")
    return(FALSE)
  })
  
  cat("All backend contract tests passed!\n")
  return(TRUE)
}

# Test our advanced CSV backend
test_params <- list(csv_file = "example_data.csv")
# test_result <- test_backend_contract("advanced_csv", test_params)
```

Systematic testing ensures your backends work correctly across different scenarios.

## Performance Optimization

Backend performance can significantly impact analysis speed, especially for large datasets:

```{r performance-optimization}
# Performance-optimized backend strategies
demonstrate_performance_patterns <- function() {
  cat("Backend performance optimization patterns:\n\n")
  
  cat("1. Lazy loading:\n")
  cat("   - Defer data loading until absolutely necessary\n")
  cat("   - Cache metadata for quick queries\n")
  cat("   - Implement partial loading for subsetting\n\n")
  
  cat("2. Memory management:\n")
  cat("   - Clear caches when backend is closed\n")
  cat("   - Monitor memory usage during operations\n")
  cat("   - Implement intelligent caching strategies\n\n")
  
  cat("3. I/O optimization:\n")
  cat("   - Minimize file system operations\n")
  cat("   - Use memory-mapped files for large datasets\n")
  cat("   - Implement efficient partial reading\n\n")
  
  cat("4. Error handling:\n")
  cat("   - Fail fast with informative messages\n")
  cat("   - Validate inputs before expensive operations\n")
  cat("   - Provide recovery suggestions\n")
}

demonstrate_performance_patterns()

# Example of performance monitoring
monitor_backend_performance <- function(backend, operations = 100) {
  if (requireNamespace("microbenchmark", quietly = TRUE)) {
    cat("Benchmarking backend operations:\n")
    
    # Benchmark dimension queries
    dim_benchmark <- microbenchmark::microbenchmark(
      dims = backend_get_dims(backend),
      times = operations
    )
    
    cat("Dimension queries: ", 
        round(mean(dim_benchmark$time) / 1e6, 2), "ms average\n")
    
    # Benchmark data access
    data_benchmark <- microbenchmark::microbenchmark(
      full_data = backend_get_data(backend),
      partial_data = backend_get_data(backend, rows = 1:10),
      times = min(operations, 10)  # Fewer iterations for data access
    )
    
    cat("Full data access: ",
        round(mean(data_benchmark$time[data_benchmark$expr == "full_data"]) / 1e6, 2), 
        "ms average\n")
    cat("Partial data access: ",
        round(mean(data_benchmark$time[data_benchmark$expr == "partial_data"]) / 1e6, 2), 
        "ms average\n")
  }
}

# Example usage (commented out for vignette)
# backend_instance <- create_backend("advanced_csv", csv_file = "example.csv")
# backend_instance <- backend_open(backend_instance)
# monitor_backend_performance(backend_instance)
```

Performance monitoring helps identify bottlenecks and optimization opportunities.

# Advanced Topics

Once you're comfortable with basic backend development, these advanced concepts help you create sophisticated, production-ready backends.

## Package Integration Strategies

For package developers, proper integration with the fmridataset ecosystem requires careful planning:

```{r package-integration}
# Example package integration pattern
demonstrate_package_integration <- function() {
  cat("Package integration best practices:\n\n")
  
  cat("1. Package initialization (.onLoad):\n")
  cat("   .onLoad <- function(libname, pkgname) {\n")
  cat("     if (requireNamespace('fmridataset', quietly = TRUE)) {\n")
  cat("       fmridataset::register_backend(\n")
  cat("         name = 'myformat',\n")
  cat("         factory = myformat_backend,\n")
  cat("         description = 'Backend for MyFormat files'\n")
  cat("       )\n")
  cat("     }\n")
  cat("   }\n\n")
  
  cat("2. Package cleanup (.onUnload):\n")
  cat("   .onUnload <- function(libpath) {\n")
  cat("     if (requireNamespace('fmridataset', quietly = TRUE)) {\n")
  cat("       fmridataset::unregister_backend('myformat')\n")
  cat("     }\n")
  cat("   }\n\n")
  
  cat("3. Conditional functionality:\n")
  cat("   - Check for fmridataset availability\n")
  cat("   - Gracefully handle missing dependencies\n")
  cat("   - Provide standalone functionality when possible\n\n")
  
  cat("4. Documentation:\n")
  cat("   - Document backend-specific parameters\n")
  cat("   - Provide usage examples\n")
  cat("   - Explain integration with fmridataset workflows\n")
}

demonstrate_package_integration()
```

Proper package integration ensures your backends work reliably across different environments.

## Advanced Validation Systems

Sophisticated backends can provide custom validation beyond the basic contract:

```{r custom-validation}
# Advanced validation for specialized backends
create_validation_system <- function(backend_name) {
  # Custom validator function
  validate_specialized_backend <- function(backend) {
    validation_results <- list()
    
    # Check 1: Data format validation
    tryCatch({
      if (backend$is_open) {
        data <- backend_get_data(backend)
        
        # Check for NaN or infinite values
        if (any(is.nan(data)) || any(is.infinite(data))) {
          validation_results$data_quality <- "FAIL: Contains NaN or infinite values"
        } else {
          validation_results$data_quality <- "PASS: Data values are valid"
        }
        
        # Check data range
        data_range <- range(data, na.rm = TRUE)
        if (diff(data_range) == 0) {
          validation_results$data_range <- "WARN: All data values are identical"
        } else if (abs(data_range[1]) > 1000 || abs(data_range[2]) > 1000) {
          validation_results$data_range <- "WARN: Data values seem unusually large"
        } else {
          validation_results$data_range <- "PASS: Data range appears reasonable"
        }
        
        # Check for temporal correlation structure
        if (nrow(data) > 10 && ncol(data) > 1) {
          temporal_corr <- cor(data[1:(nrow(data)-1), ], data[2:nrow(data), ])
          mean_temporal_corr <- mean(diag(temporal_corr), na.rm = TRUE)
          
          if (mean_temporal_corr < 0.1) {
            validation_results$temporal_structure <- 
              "WARN: Low temporal correlation (may indicate noise)"
          } else if (mean_temporal_corr > 0.95) {
            validation_results$temporal_structure <- 
              "WARN: Very high temporal correlation (may indicate processing artifact)"
          } else {
            validation_results$temporal_structure <- 
              "PASS: Temporal correlation structure appears normal"
          }
        }
      }
      
    }, error = function(e) {
      validation_results$data_access <- paste("ERROR:", conditionMessage(e))
    })
    
    # Check 2: Metadata consistency
    tryCatch({
      metadata <- backend_get_metadata(backend)
      dims <- backend_get_dims(backend)
      
      # Validate metadata completeness
      required_fields <- c("format", "source_file")
      missing_fields <- setdiff(required_fields, names(metadata))
      
      if (length(missing_fields) > 0) {
        validation_results$metadata_completeness <- 
          paste("WARN: Missing metadata fields:", paste(missing_fields, collapse = ", "))
      } else {
        validation_results$metadata_completeness <- "PASS: All required metadata present"
      }
      
    }, error = function(e) {
      validation_results$metadata_access <- paste("ERROR:", conditionMessage(e))
    })
    
    return(validation_results)
  }
  
  # Register custom validator
  cat("Custom validation system created for:", backend_name, "\n")
  cat("Validation checks:\n")
  cat("  - Data quality (NaN, infinite values)\n")
  cat("  - Data range reasonableness\n")
  cat("  - Temporal correlation structure\n")
  cat("  - Metadata completeness\n")
  
  return(validate_specialized_backend)
}

# Example usage
csv_validator <- create_validation_system("advanced_csv")

# Apply validation to a backend
validate_backend_instance <- function(backend) {
  cat("Running custom validation...\n")
  
  validation_results <- csv_validator(backend)
  
  for (check_name in names(validation_results)) {
    result <- validation_results[[check_name]]
    status <- substr(result, 1, 4)
    
    if (status == "PASS") {
      cat("✓", check_name, ":", result, "\n")
    } else if (status == "WARN") {
      cat("⚠", check_name, ":", result, "\n")
    } else {
      cat("✗", check_name, ":", result, "\n")
    }
  }
}

# validate_backend_instance(backend_instance)
```

Custom validation helps ensure data quality and catch format-specific issues.

## Backend Composition and Chaining

Advanced scenarios may require composing multiple backends or creating backend chains:

```{r backend-composition}
# Example: Composite backend that combines multiple sources
create_composite_backend <- function(backend_list, combination_strategy = "concatenate") {
  
  composite_backend <- function(...) {
    # Validate all component backends
    for (i in seq_along(backend_list)) {
      if (!inherits(backend_list[[i]], "storage_backend")) {
        stop("Component ", i, " is not a valid backend")
      }
    }
    
    backend <- list(
      component_backends = backend_list,
      combination_strategy = combination_strategy,
      is_open = FALSE,
      combined_dims = NULL,
      combined_mask = NULL
    )
    
    class(backend) <- c("composite_backend", "storage_backend")
    backend
  }
  
  return(composite_backend)
}

# Implement composite backend methods
backend_open.composite_backend <- function(backend) {
  # Open all component backends
  for (i in seq_along(backend$component_backends)) {
    backend$component_backends[[i]] <- backend_open(backend$component_backends[[i]])
  }
  
  # Compute combined dimensions and masks
  backend$is_open <- TRUE
  
  cat("Opened composite backend with", length(backend$component_backends), "components\n")
  backend
}

backend_get_dims.composite_backend <- function(backend) {
  if (!backend$is_open) {
    stop("Composite backend must be opened first")
  }
  
  # Combine dimensions based on strategy
  component_dims <- lapply(backend$component_backends, backend_get_dims)
  
  if (backend$combination_strategy == "concatenate") {
    # Concatenate timepoints, ensure spatial dimensions match
    total_time <- sum(sapply(component_dims, function(x) x$time))
    spatial_dims <- component_dims[[1]]$spatial
    
    # Validate spatial consistency
    for (dims in component_dims[-1]) {
      if (!identical(dims$spatial, spatial_dims)) {
        stop("Spatial dimensions must match for concatenation")
      }
    }
    
    return(list(spatial = spatial_dims, time = total_time))
  }
  
  # Add other combination strategies as needed
  stop("Unsupported combination strategy: ", backend$combination_strategy)
}

# Additional composite backend methods would follow similar patterns...

cat("Composite backend framework created\n")
cat("Supports combination strategies for multiple data sources\n")
```

Composite backends enable sophisticated data integration scenarios.

# Tips and Best Practices

Here are practical guidelines learned from developing and maintaining production backends that will help you create robust, reliable extensions.

> **⚠️ Performance Tip**: Always implement lazy loading in your backends. Loading data during backend creation can cause memory issues and slow dataset creation, especially for large files.

> **🛡️ Best Practice**: Implement comprehensive input validation with clear error messages. Users should immediately understand what went wrong and how to fix it, rather than encountering mysterious failures later.

> **⚡ Pro Tip**: Use the registry's introspection capabilities to check for backend availability before using specialized features. This makes your code more robust across different environments.

## Development Workflow

Effective backend development follows a systematic workflow:

```{r development-workflow}
backend_development_checklist <- function() {
  cat("Backend development checklist:\n\n")
  
  cat("1. Planning phase:\n")
  cat("   □ Understand the data format thoroughly\n")
  cat("   □ Identify performance requirements\n")
  cat("   □ Plan for edge cases and error conditions\n")
  cat("   □ Design the user interface (constructor parameters)\n\n")
  
  cat("2. Implementation phase:\n")
  cat("   □ Create constructor function with input validation\n")
  cat("   □ Implement all six required contract methods\n")
  cat("   □ Add comprehensive error handling\n")
  cat("   □ Implement resource management (open/close)\n\n")
  
  cat("3. Testing phase:\n")
  cat("   □ Test with valid inputs\n")
  cat("   □ Test with invalid inputs (error handling)\n")
  cat("   □ Test edge cases (empty files, large files, etc.)\n")
  cat("   □ Test performance characteristics\n\n")
  
  cat("4. Integration phase:\n")
  cat("   □ Register backend with descriptive name\n")
  cat("   □ Test integration with existing workflows\n")
  cat("   □ Validate chunking behavior\n")
  cat("   □ Test with study-level operations\n\n")
  
  cat("5. Documentation phase:\n")
  cat("   □ Document constructor parameters\n")
  cat("   □ Provide usage examples\n")
  cat("   □ Document known limitations\n")
  cat("   □ Create troubleshooting guide\n")
}

backend_development_checklist()
```

## Error Handling Strategies

Robust error handling is crucial for production backends:

```{r error-handling-strategies}
demonstrate_error_handling <- function() {
  cat("Error handling best practices:\n\n")
  
  cat("1. Fail fast principle:\n")
  cat("   - Validate inputs immediately\n")
  cat("   - Check file existence before attempting operations\n")
  cat("   - Verify data format early in the process\n\n")
  
  cat("2. Informative error messages:\n")
  cat("   - Explain what went wrong\n")
  cat("   - Suggest corrective actions\n")
  cat("   - Include relevant context (file paths, data dimensions)\n\n")
  
  cat("3. Graceful degradation:\n")
  cat("   - Handle partial failures when possible\n")
  cat("   - Provide warnings for non-critical issues\n")
  cat("   - Clean up resources on failure\n\n")
  
  cat("4. Consistent error types:\n")
  cat("   - Use appropriate error classes\n")
  cat("   - Follow R error handling conventions\n")
  cat("   - Provide machine-readable error information\n")
}

demonstrate_error_handling()

# Example of good error handling
robust_error_handling_example <- function(file_path) {
  # Good: Check file existence with helpful message
  if (!file.exists(file_path)) {
    stop("File not found: '", file_path, 
         "'. Please check the path and ensure the file exists.")
  }
  
  # Good: Check file permissions
  if (!file.access(file_path, mode = 4) == 0) {
    stop("Cannot read file: '", file_path, 
         "'. Please check file permissions.")
  }
  
  # Good: Validate file format before processing
  file_ext <- tools::file_ext(file_path)
  if (!file_ext %in% c("csv", "txt")) {
    stop("Unsupported file format: '.", file_ext, 
         "'. Expected .csv or .txt files.")
  }
  
  cat("File validation passed for:", file_path, "\n")
}
```

## Testing and Validation

Comprehensive testing ensures backend reliability:

```{r testing-strategies}
create_backend_test_suite <- function(backend_name) {
  cat("Creating test suite for:", backend_name, "\n\n")
  
  test_scenarios <- list(
    "normal_operation" = list(
      description = "Test normal backend operations",
      test_data = "valid_file.csv",
      expected_result = "success"
    ),
    
    "missing_file" = list(
      description = "Test handling of missing files",
      test_data = "nonexistent_file.csv",
      expected_result = "error"
    ),
    
    "corrupted_data" = list(
      description = "Test handling of corrupted data",
      test_data = "corrupted_file.csv",
      expected_result = "error"
    ),
    
    "large_file" = list(
      description = "Test performance with large files",
      test_data = "large_file.csv",
      expected_result = "success_with_warning"
    ),
    
    "edge_cases" = list(
      description = "Test edge cases (empty files, single values)",
      test_data = "edge_case_file.csv",
      expected_result = "success_or_documented_limitation"
    )
  )
  
  cat("Test scenarios defined:\n")
  for (scenario_name in names(test_scenarios)) {
    scenario <- test_scenarios[[scenario_name]]
    cat("  -", scenario_name, ":", scenario$description, "\n")
  }
  
  cat("\nRun these tests systematically during development\n")
  return(test_scenarios)
}

# create_test_suite <- create_backend_test_suite("advanced_csv")
```

# Troubleshooting Backend Issues

When developing or using custom backends, certain issues are common and can be systematically diagnosed and resolved.

## Common Development Issues

**"Error: Backend must implement method X"**
: This indicates missing required methods in your backend implementation. Ensure all six contract methods are implemented: `backend_open`, `backend_close`, `backend_get_dims`, `backend_get_mask`, `backend_get_data`, and `backend_get_metadata`.

**"Error: Backend registration failed"**
: The registry validation detected issues with your backend. Check that your factory function returns an object with the correct class inheritance and that all methods are properly defined.

**Memory leaks during backend operations**
: Ensure your `backend_close` method properly cleans up cached data and releases resources. Implement explicit memory management in long-running operations.

```{r troubleshooting-development}
# Diagnostic tools for backend development
diagnose_backend_issues <- function(backend_name) {
  cat("Diagnosing backend issues for:", backend_name, "\n\n")
  
  # Check if backend is registered
  if (!is_backend_registered(backend_name)) {
    cat("✗ Backend not registered\n")
    cat("  Solution: Call register_backend() with your backend\n")
    return()
  } else {
    cat("✓ Backend is registered\n")
  }
  
  # Check backend info
  tryCatch({
    backend_info <- get_backend_registry(backend_name)
    cat("✓ Backend info accessible\n")
    cat("  Description:", backend_info$description, "\n")
  }, error = function(e) {
    cat("✗ Cannot access backend info:", conditionMessage(e), "\n")
  })
  
  # Test backend creation with minimal parameters
  cat("\nTesting backend creation...\n")
  # This would test actual backend creation in practice
  
  cat("Diagnosis complete\n")
}

# Example troubleshooting function
troubleshoot_backend_errors <- function(error_message) {
  cat("Troubleshooting backend error:\n")
  cat("Error:", error_message, "\n\n")
  
  if (grepl("not found", error_message, ignore.case = TRUE)) {
    cat("Likely cause: File path issue\n")
    cat("Solutions:\n")
    cat("  - Check file exists with file.exists()\n")
    cat("  - Use absolute paths\n")
    cat("  - Verify file permissions\n")
  } else if (grepl("implement.*method", error_message, ignore.case = TRUE)) {
    cat("Likely cause: Missing backend method\n")
    cat("Solutions:\n")
    cat("  - Implement all required contract methods\n")
    cat("  - Check method naming (backend_open, backend_close, etc.)\n")
    cat("  - Verify S3 method registration\n")
  } else if (grepl("dimensions", error_message, ignore.case = TRUE)) {
    cat("Likely cause: Dimension mismatch\n")
    cat("Solutions:\n")
    cat("  - Validate data dimensions in backend_open\n")
    cat("  - Check mask length matches data columns\n")
    cat("  - Ensure consistent spatial dimensions\n")
  } else {
    cat("General debugging steps:\n")
    cat("  - Check backend registration\n")
    cat("  - Validate input parameters\n")
    cat("  - Test with minimal example\n")
    cat("  - Check error logs for more details\n")
  }
}
```

## Performance Issues

Backend performance problems often stem from inefficient I/O or memory management:

```{r performance-troubleshooting}
# Performance diagnostic tools
profile_backend_performance <- function(backend_name, test_file) {
  cat("Profiling backend performance:", backend_name, "\n")
  
  if (requireNamespace("profvis", quietly = TRUE)) {
    cat("Using profvis for detailed profiling\n")
    # In practice, you would use profvis::profvis() here
  }
  
  # Basic timing measurements
  backend_creation_time <- system.time({
    # backend <- create_backend(backend_name, csv_file = test_file)
  })
  
  cat("Backend creation time:", backend_creation_time["elapsed"], "seconds\n")
  
  # Memory usage tracking
  if (requireNamespace("pryr", quietly = TRUE)) {
    cat("Memory usage tracking available\n")
    # Track memory during operations
  }
  
  cat("Performance profiling complete\n")
}

# Identify common performance bottlenecks
identify_performance_bottlenecks <- function() {
  cat("Common backend performance bottlenecks:\n\n")
  
  cat("1. Eager data loading:\n")
  cat("   - Loading all data during backend_open\n")
  cat("   - Solution: Implement lazy loading\n\n")
  
  cat("2. Inefficient file I/O:\n")
  cat("   - Reading entire files for partial access\n")
  cat("   - Solution: Implement partial reading\n\n")
  
  cat("3. Memory leaks:\n")
  cat("   - Not clearing caches on close\n")
  cat("   - Solution: Explicit memory management\n\n")
  
  cat("4. Repeated operations:\n")
  cat("   - Re-reading metadata on each access\n")
  cat("   - Solution: Intelligent caching\n")
}

identify_performance_bottlenecks()
```

# Integration with Other Vignettes

This backend registry guide connects to several other aspects of the fmridataset ecosystem:

**Prerequisites**: Understanding the [Architecture Overview](architecture-overview.html) helps you appreciate how backends fit into the overall system design.

**Practical Application**: The [Getting Started](fmridataset-intro.html) guide shows how different backends work from a user perspective.

**Advanced Usage**:
- [Extending Backends](extending-backends.html) - Deep dive into sophisticated backend development patterns
- [H5 Backend Usage](h5-backend-usage.html) - Example of a production-quality backend implementation
- [Study-Level Analysis](study-level-analysis.html) - See how custom backends work with multi-subject studies

**Development Context**: The backend registry system exemplifies key principles of modular software design that are common throughout the neuroimaging ecosystem. Understanding these patterns will help you work effectively with other extensible packages.

**Package Development**: If you're developing packages that work with neuroimaging data, the backend registry pattern provides a template for creating extensible, interoperable systems.

# Session Information

```{r session-info}
sessionInfo()
```